\documentclass{article}

\usepackage[preprint,nonatbib]{neurips_2025}
\usepackage[numbers,sort&compress]{natbib}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{float}  % For [H] placement
\graphicspath{{./asset/figure/}}

% Custom commands
\newcommand{\astral}{\textsc{Astral}}
\newcommand{\method}{\astral}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bA}{\mathbf{A}}

\title{ASTRAL: Abstraction-Structured Test-time Reinforcement Adaptation Layer for Non-Stationary Environments}

\author{%
  Anonymous Author(s) \\
  Institution \\
  \texttt{anonymous@institution.edu}
}

\begin{document}

\maketitle

\begin{abstract}
Reinforcement learning agents deployed in non-stationary environments must adapt to changing dynamics without catastrophically forgetting previously learned behaviors. We introduce \method{} (\textbf{A}bstraction-\textbf{S}tructured \textbf{T}est-time \textbf{R}einforcement \textbf{A}daptation \textbf{L}ayer), an architecture that maintains a bank of learnable abstraction vectors combined through a lightweight gating mechanism. At test time, only the gating network ($\sim$4,300 parameters) is adapted while the policy remains frozen, providing a structural guarantee against forgetting. Through extensive experiments on a non-stationary CartPole environment with three physics modes, we make several contributions: (1) we discover and characterize \emph{slot collapse}, where abstraction banks converge to using a single slot; (2) we show that \emph{slot dropout} during training mitigates collapse and enables effective test-time adaptation; (3) through fair comparison experiments, we demonstrate that \method{}'s gating-only adaptation achieves \textbf{10$\times$ less catastrophic forgetting} compared to full fine-tuning ($-25.8$ vs $-250.1$); and (4) we show that gating adaptation maintains consistent, low-variance performance across all episode budgets (1-50), while policy-head and full fine-tuning exhibit high variance with frequent catastrophic failures. Our results suggest that \method{} is particularly suited for \emph{risk-averse} deployment scenarios where stable adaptation across multiple environment modes is more critical than maximizing single-mode performance.
\end{abstract}

%==============================================================================
\section{Introduction}
\label{sec:intro}
%==============================================================================

Real-world reinforcement learning (RL) applications must contend with non-stationary environments where the underlying dynamics can change over time \citep{padakandla2020survey, xie2020deep}. A robot trained in simulation may encounter different friction coefficients when deployed in the real world \citep{tobin2017domain, kumar2021rma}. An autonomous vehicle must adapt to varying weather conditions. A game-playing agent may face opponents with changing strategies. Traditional RL algorithms assume stationary Markov Decision Processes (MDPs) \citep{hallak2015contextual}, making them brittle when environment dynamics shift.

Existing approaches to non-stationarity fall into several categories: (1) \emph{domain randomization} \citep{tobin2017domain}, which trains on diverse conditions hoping to learn robust policies; (2) \emph{meta-learning} \citep{finn2017maml, nichol2018firstorder, rakelly2019efficient}, which explicitly optimizes for fast adaptation; and (3) \emph{context-conditioned policies} \citep{hallak2015contextual, lee2020context}, which learn to condition behavior on inferred environment parameters. However, these approaches share a critical limitation: when adapting to a new mode, they risk \emph{catastrophic forgetting} \citep{mccloskey1989catastrophic, french1999catastrophic, kirkpatrick2017overcoming}---destroying performance on previously learned modes.

A fundamental but often overlooked consideration is \emph{how} adaptation affects performance on other environment modes \citep{parisi2019continual}. An agent that perfectly adapts to Mode A but forgets Mode B may be worse than an agent that moderately handles both. This is especially critical in multi-modal deployments where the agent must maintain competence across all conditions simultaneously \citep{rolnick2019experience}.

We propose \method{}, an architecture designed for \textbf{stable, forgetting-resistant adaptation} to non-stationary environments. The key insight is to decompose the agent's internal representation into a bank of discrete \emph{abstraction slots}, each potentially capturing a different behavioral mode. A lightweight gating network determines which abstractions to use based on the current context. Crucially, at test time, only this gating network is adapted, enabling:

\begin{itemize}
    \item \textbf{Structural forgetting resistance}: With the policy frozen, adaptation \emph{cannot} catastrophically destroy learned behaviors---the gating network can only reweight existing abstractions.
    \item \textbf{Efficiency}: Only $\sim$4,300 trainable parameters (8\% of total) are updated during adaptation.
    \item \textbf{Predictability}: Low-variance adaptation performance across all episode budgets.
    \item \textbf{Interpretability}: The gating weights reveal which abstraction the agent is using, potentially corresponding to identified environment modes.
\end{itemize}

Through extensive experimentation (33+ models, 4 fair comparison experiments), we make the following contributions:

\begin{enumerate}
    \item We introduce the \method{} architecture combining a GRU backbone with an abstraction bank and FiLM modulation (Section~\ref{sec:method}).
    \item We discover and characterize the \emph{slot collapse} phenomenon where abstraction slots fail to specialize, and show that \emph{slot dropout} mitigates this problem (Section~\ref{sec:collapse}).
    \item We demonstrate through fair comparison experiments that gating-only adaptation provides \textbf{10$\times$ less forgetting} than full fine-tuning while maintaining stable performance across varying adaptation budgets (Section~\ref{sec:fair_comparison}).
    \item We provide analysis of when \method{} is preferable to standard fine-tuning, identifying risk-averse multi-mode deployment as the key use case (Section~\ref{sec:analysis}).
\end{enumerate}

%==============================================================================
\section{Related Work}
\label{sec:related}
%==============================================================================

Our work sits at the intersection of several research areas: non-stationary reinforcement learning, abstraction learning, test-time adaptation, and catastrophic forgetting prevention. We review each area and position \method{} within this landscape.

\subsection{Non-Stationary and Hidden-Mode MDPs}

Non-stationary Markov Decision Processes (MDPs) relax the standard assumption of fixed transition dynamics, allowing the environment to change over time \citep{padakandla2020survey}. Hidden-mode MDPs \citep{choi2000hidden} formalize this as a finite set of latent modes governing dynamics, with mode transitions following a separate stochastic process. Early work by \citet{da2006dealing} proposed maintaining separate value functions for different modes, while \citet{hallak2015contextual} introduced context-conditioned policies that learn to infer and condition on latent context variables.

More recent approaches include LILAC \citep{xie2020deep}, which learns latent representations of environment dynamics for transfer, and variational approaches \citep{zintgraf2020varibad} that maintain beliefs over task identity. Our work differs fundamentally: rather than inferring mode identity, we learn a bank of abstractions that can be \emph{recombined} through gating weights, allowing smooth interpolation between learned behaviors.

\subsection{Abstraction and State Representation in RL}

The role of abstraction in RL has deep roots in state abstraction theory \citep{li2006towards}, which studies how to aggregate states while preserving optimal behavior. Bisimulation metrics \citep{ferns2004metrics, castro2010using} provide theoretical foundations for learning abstract representations that preserve value equivalence. Recent deep learning approaches include DeepMDP \citep{gelada2019deepmdp}, which learns representations predictive of rewards and transitions, and contrastive methods \citep{laskin2020curl} that learn representations invariant to task-irrelevant factors.

Object-centric and slot-based representations have emerged as powerful abstractions for visual RL. Slot Attention \citep{locatello2020object} learns to decompose scenes into object slots, while Entity Abstraction \citep{veerapaneni2020entity} applies this to RL for better generalization. Our abstraction bank is conceptually related but operates at the \emph{behavioral} level---slots represent behavioral modes rather than visual objects---and we introduce a novel mechanism for test-time adaptation through gating.

\subsection{Catastrophic Forgetting and Continual Learning}

Catastrophic forgetting \citep{mccloskey1989catastrophic, french1999catastrophic} occurs when neural networks lose previously learned knowledge upon training on new tasks. This fundamental challenge has spawned extensive research in continual learning \citep{parisi2019continual}. Three main approaches have emerged:

\paragraph{Regularization-based methods} constrain weight updates to preserve important parameters. Elastic Weight Consolidation (EWC) \citep{kirkpatrick2017overcoming} uses Fisher information to identify important weights. Synaptic Intelligence \citep{zenke2017continual} tracks parameter importance online. Memory Aware Synapses \citep{aljundi2018memory} improves importance estimation.

\paragraph{Replay-based methods} store and rehearse past experiences. Experience Replay \citep{rolnick2019experience} maintains a buffer of past transitions. Generative replay \citep{shin2017continual} uses generative models to synthesize past experiences.

\paragraph{Architecture-based methods} allocate separate parameters for different tasks. Progressive Neural Networks \citep{rusu2016progressive} add new columns for new tasks with lateral connections. PackNet \citep{mallya2018packnet} iteratively prunes and freezes parameters. Modular networks \citep{andreas2016neural} compose task-specific modules.

\method{} contributes a new architectural approach: by \emph{freezing the policy} and adapting only gating weights, we provide a \emph{structural guarantee} against forgetting. Unlike regularization methods that still risk drift, our approach fundamentally cannot corrupt learned behaviors.

\subsection{Test-Time Adaptation}

Test-time adaptation (TTA) has gained significant attention in computer vision, where models must adapt to distribution shifts without access to source data. Tent \citep{wang2021tent} adapts batch normalization statistics using entropy minimization. MEMO \citep{zhang2022memo} uses augmentation-based self-supervision. TTT \citep{sun2020test} trains auxiliary self-supervised tasks that can be optimized at test time.

In RL, test-time adaptation is less explored. Most work focuses on \emph{training-time} adaptation through meta-learning \citep{finn2017maml} or domain randomization \citep{tobin2017domain}. Recent work on adaptive RL includes rapid motor adaptation \citep{kumar2021rma}, which learns adaptation modules for robotics, and context-based adaptation \citep{lee2020context} for sim-to-real transfer.

Our work introduces a principled TTA framework for RL: adapt a minimal set of parameters (gating network) while freezing the policy. This is analogous to adapting only batch norm parameters in vision TTA, but with the additional benefit of structural forgetting resistance.

\subsection{Meta-Learning for Fast Adaptation}

Meta-learning approaches explicitly optimize for fast adaptation. Model-Agnostic Meta-Learning (MAML) \citep{finn2017maml} learns initializations that enable few-shot fine-tuning. Reptile \citep{nichol2018firstorder} provides a first-order approximation. RL$^2$ \citep{duan2016rl2} and related in-context learning approaches \citep{wang2016learning, mishra2018simple} train recurrent networks that adapt within episodes through their hidden states.

Gradient-based meta-RL methods include MAML-RL \citep{finn2017maml}, ProMP \citep{rothfuss2019promp}, and PEARL \citep{rakelly2019efficient}. These methods optimize for fast fine-tuning but do not explicitly address forgetting during adaptation. Our approach trades maximum adaptation capability for stability: we cannot achieve the same peak improvement as full fine-tuning, but we guarantee no catastrophic forgetting.

\subsection{Mixture of Experts and Gating Mechanisms}

Mixture of Experts (MoE) \citep{jacobs1991adaptive} routes inputs to specialized sub-networks through a gating function. The Sparsely-Gated MoE \citep{shazeer2017outrageously} scaled this to billions of parameters using top-k routing. Recent work has applied MoE to RL: \citet{ren2021probabilistic} use probabilistic mixture policies, and \citet{goyal2019reinforcement} combine MoE with option frameworks.

Our abstraction bank differs from traditional MoE in several ways: (1) we use soft attention over \emph{abstraction vectors} rather than routing to separate networks; (2) we modulate a shared policy through FiLM rather than combining expert outputs; (3) crucially, we adapt only the gating network at test time while keeping abstractions frozen.

The gating mechanism itself relates to attention \citep{bahdanau2015neural, vaswani2017attention}. Our gating network computes attention weights over slots, similar to cross-attention but applied to learned embeddings rather than input tokens.

\subsection{Feature Modulation}

Feature-wise Linear Modulation (FiLM) \citep{perez2018film} modulates neural network activations through learned scale ($\gamma$) and shift ($\beta$) parameters: $\text{FiLM}(\mathbf{h}) = \gamma \odot \mathbf{h} + \beta$. Originally proposed for visual reasoning, FiLM has been applied to RL for goal conditioning \citep{barreto2020fast}, multi-task learning \citep{sodhani2021multi}, and skill composition \citep{peng2019mcp}.

We use FiLM to inject abstraction information into the policy: the weighted combination of slot vectors produces FiLM parameters that modulate the recurrent hidden state. This allows the abstraction bank to influence behavior without modifying the policy network directly, enabling our frozen-policy TTA approach.

\subsection{Dropout and Stochastic Regularization}

Dropout \citep{srivastava2014dropout} randomly zeroes activations during training to prevent overfitting. Variants include DropConnect \citep{wan2013regularization} for weights and Spatial Dropout \citep{tompson2015efficient} for convolutional features. In attention mechanisms, DropHead \citep{zhou2020scheduled} drops entire attention heads.

Our \textbf{slot dropout} is conceptually related but serves a different purpose: preventing \emph{slot collapse} rather than overfitting. By randomly masking slots during training, we force the network to learn useful representations in all slots, ensuring meaningful test-time adaptation through gating weight changes.

%==============================================================================
\section{Method}
\label{sec:method}
%==============================================================================

\subsection{Problem Setting}

We consider a non-stationary MDP \citep{padakandla2020survey} $\mathcal{M} = (\mathcal{S}, \mathcal{A}, \{T_m\}_{m=1}^M, \{R_m\}_{m=1}^M, \gamma)$ where the transition dynamics $T_m: \mathcal{S} \times \mathcal{A} \rightarrow \Delta(\mathcal{S})$ and reward function $R_m: \mathcal{S} \times \mathcal{A} \rightarrow \R$ depend on an unobserved mode $m \in \{1, \ldots, M\}$, following the hidden-mode MDP formulation \citep{choi2000hidden}. The mode may change between episodes. The agent must learn a policy that performs well across all modes during training, and can quickly adapt when the mode distribution shifts at test time---without forgetting previously learned modes \citep{parisi2019continual}.

\subsection{Architecture Overview}

\method{} consists of four components (Figure~\ref{fig:architecture}):

\begin{enumerate}
    \item \textbf{Input Projection}: Maps observation $o_t$, previous action $a_{t-1}$, and reward $r_{t-1}$ to embedding $\mathbf{e}_t \in \R^d$.
    \item \textbf{Recurrent Backbone}: A GRU that maintains hidden state $\bh_t \in \R^d$ capturing temporal context: $\bh_t = \text{GRU}(\bh_{t-1}, \mathbf{e}_t)$.
    \item \textbf{Abstraction Bank}: Produces abstraction $\bz_t$ and weights $\bw_t$ from context $\bh_t$.
    \item \textbf{FiLM-Modulated Heads}: Policy and value heads that are modulated by the abstraction.
\end{enumerate}

\begin{figure}[H]
\centering
\hspace*{-0.03\textwidth}\includegraphics[width=1.1\textwidth]{architecture.png}
\caption{\textbf{ASTRAL Architecture.} The agent processes observations through a GRU backbone. The Abstraction Bank maintains $K$ learnable slot vectors combined via a gating network. At test time, \textbf{only the gating network} (green, $\sim$4.3k parameters) is adapted, while all other components remain frozen.}
\label{fig:architecture}
\end{figure}

\subsection{Abstraction Bank}

The abstraction bank maintains $K$ learnable abstraction vectors $\bA = [\mathbf{a}_1, \ldots, \mathbf{a}_K] \in \R^{K \times d}$, inspired by slot-based representations \citep{locatello2020object} but applied to behavioral rather than visual abstraction. Given context $\bh_t$, the gating network computes attention weights \citep{bahdanau2015neural, vaswani2017attention} over slots:
\begin{align}
    \text{logits} &= W_g \cdot \text{ReLU}(W_h \bh_t + b_h) + b_g \\
    \bw_t &= \text{softmax}(\text{logits} / \tau) \\
    \bz_t &= \sum_{k=1}^K w_{t,k} \mathbf{a}_k
\end{align}
where $\tau$ is a temperature parameter controlling the sharpness of the attention distribution. The gating network parameters $\{W_g, W_h, b_g, b_h\}$ comprise approximately 4,300 parameters---the only parameters updated during test-time adaptation.

\subsection{FiLM Modulation}

Following \citet{perez2018film}, we use Feature-wise Linear Modulation to incorporate the abstraction into the policy computation:
\begin{align}
    \gamma, \beta &= \text{MLP}_{\text{FiLM}}(\bz_t) \\
    \bh'_t &= \gamma \odot \bh_t + \beta
\end{align}
The modulated representation $\bh'_t$ is then passed to policy and value heads to produce action probabilities $\pi(a|s)$ and value estimate $V(s)$.

\subsection{Training Objective}

We train using Proximal Policy Optimization (PPO) \citep{schulman2017proximal} with additional regularization losses to encourage slot diversity:
\begin{align}
    \mathcal{L} = \mathcal{L}_{\text{PPO}} + \lambda_{\text{ent}} \mathcal{L}_{\text{w-ent}} + \lambda_{\text{lb}} \mathcal{L}_{\text{lb}} + \lambda_{\text{con}} \mathcal{L}_{\text{contrast}}
\end{align}

\paragraph{Weight Entropy ($\mathcal{L}_{\text{w-ent}}$).} Encourages diverse slot usage by maximizing the entropy of average slot weights:
\begin{align}
    \mathcal{L}_{\text{w-ent}} = -\sum_k \bar{w}_k \log(\bar{w}_k + \epsilon)
\end{align}
where $\bar{w}_k$ is the mean weight for slot $k$ across the batch.

\paragraph{Load Balancing ($\mathcal{L}_{\text{lb}}$).} Following \citet{shazeer2017outrageously}, encourages equal slot utilization:
\begin{align}
    \mathcal{L}_{\text{lb}} = K \cdot \sum_k f_k \cdot \bar{w}_k
\end{align}
where $f_k$ is the fraction of samples primarily routed to slot $k$.

\paragraph{Contrastive ($\mathcal{L}_{\text{contrast}}$).} Encourages mode-slot correspondence by pushing weight vectors from the same mode together:
\begin{align}
    \mathcal{L}_{\text{contrast}} = -\log \frac{\exp(\bw_t^T \bw_{t'} / \tau)}{\sum_{t'' \in \text{batch}} \exp(\bw_t^T \bw_{t''} / \tau)}
\end{align}
where $t$ and $t'$ are timesteps from the same environment mode.

\paragraph{Slot Dropout.} Despite regularization, we found that models still exhibit \emph{slot collapse} (Section~\ref{sec:collapse}). Inspired by dropout regularization \citep{srivastava2014dropout} and attention dropout techniques \citep{zhou2020scheduled}, our key innovation is \textbf{slot dropout}: during training, each slot weight is zeroed with probability $p$ and weights are renormalized:
\begin{align}
    \tilde{w}_{t,k} &= w_{t,k} \cdot \mathbf{1}[u_k > p], \quad u_k \sim \text{Uniform}(0,1) \\
    \hat{w}_{t,k} &= \tilde{w}_{t,k} / (\sum_j \tilde{w}_{t,j} + \epsilon)
\end{align}
This forces the network to learn useful representations in \emph{all} slots, since any slot may be masked during training.

\subsection{Test-Time Adaptation}

At test time, we freeze all parameters except the gating network $\theta_g = \{W_g, W_h, b_g, b_h\}$ and adapt using REINFORCE \citep{schulman2017proximal}:
\begin{align}
    \nabla_{\theta_g} J(\theta_g) \approx \sum_t \nabla_{\theta_g} \log \pi_{\theta_g}(a_t|s_t) \cdot G_t
\end{align}
where $G_t$ is the return from timestep $t$. This adapts only $\sim$4,300 parameters while preserving the learned policy, providing a structural guarantee against catastrophic forgetting \citep{kirkpatrick2017overcoming, rusu2016progressive}. Unlike regularization-based approaches to forgetting prevention \citep{zenke2017continual, aljundi2018memory}, our approach provides an architectural guarantee: the policy \emph{cannot} be corrupted because its parameters are never updated.

%==============================================================================
\section{Experimental Setup}
\label{sec:setup}
%==============================================================================

\subsection{Environment: Non-Stationary CartPole}

We modify the classic CartPole environment \citep{brockman2016gym} to include three distinct physics modes:

\begin{table}[h]
\centering
\caption{Non-Stationary CartPole Environment Modes}
\label{tab:env_modes}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Mode} & \textbf{Gravity} & \textbf{Pole Length} & \textbf{Pole Mass} \\
\midrule
0 (Standard) & 9.8 & 0.5 & 0.1 \\
1 (Long Pole) & 10.8 & 0.6 & 0.1 \\
2 (Heavy Pole) & 9.8 & 0.4 & 0.2 \\
\bottomrule
\end{tabular}
\end{table}

The environment randomly switches modes at episode boundaries. Episodes terminate after 500 steps (maximum return) or if the pole falls.

\subsection{Implementation Details}

We implement \method{} in PyTorch. Key hyperparameters: $K=3$ slots, $d=64$ hidden dimension, learning rate $3 \times 10^{-4}$, $\gamma=0.99$, $\tau=1.0$ temperature. Each model has approximately 51,000 parameters, with the gating network containing $\sim$4,300 trainable parameters for test-time adaptation. Training uses 300,000 timesteps ($\sim$3 minutes on GPU).

\subsection{Baselines}

We compare against:
\begin{itemize}
    \item \textbf{SB3 PPO}: Stable-Baselines3 PPO \citep{stable-baselines3} trained on the same environment (100k steps, $\sim$443 mean return). This serves as a strong, well-tuned baseline.
    \item \textbf{Full Fine-Tuning}: Adapting all \method{} parameters at test time ($\sim$51k params).
    \item \textbf{Policy-Head Fine-Tuning}: Adapting only the policy head ($\sim$4,300 params, parameter-matched to gating).
\end{itemize}

\subsection{Adaptation Protocol}

For each adaptation experiment:
\begin{enumerate}
    \item Evaluate on target mode (20 episodes) $\rightarrow$ ``Before'' score
    \item Adapt for $N$ episodes on target mode using the specified method
    \item Evaluate again (20 episodes) $\rightarrow$ ``After'' score
    \item For forgetting tests: also evaluate on non-adapted modes
\end{enumerate}

%==============================================================================
\section{The Slot Collapse Problem}
\label{sec:collapse}
%==============================================================================

\subsection{Phenomenon}

During initial experiments, we observed that \method{} consistently converged to using a single abstraction slot, regardless of the environment mode (Figure~\ref{fig:slot_collapse}). Across 11 configurations tested, 8 (73\%) exhibited severe collapse with $>$95\% weight on a single slot.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{slot_collapse_comparison.png}
\caption{\textbf{Slot Collapse Problem and Solution.} Left: Without slot dropout, weights collapse to a single slot (Slot 1 = 100\%). Right: With slot dropout ($p=0.3$), weights are distributed across all slots, enabling meaningful test-time adaptation.}
\label{fig:slot_collapse}
\end{figure}

\subsection{Analysis}

We identify several factors contributing to slot collapse, which parallels challenges observed in Mixture of Experts routing \citep{shazeer2017outrageously} and attention mechanisms more broadly \citep{vaswani2017attention}:

\paragraph{Policy Gradient Reinforcement.} Once a slot achieves slightly better performance, the policy gradient reinforces its usage, creating a positive feedback loop that suppresses other slots. This is analogous to the ``rich-get-richer'' phenomenon in attention-based models \citep{bahdanau2015neural}.

\paragraph{Soft Attention Convergence.} The softmax operation naturally converges to near-one-hot distributions as the gating network learns to maximize return. Similar collapse has been observed in MoE models, motivating load balancing losses \citep{shazeer2017outrageously}.

\paragraph{Mode Similarity.} The three CartPole modes, while requiring different control strategies, may be similar enough that a single abstraction can achieve reasonable performance across all. This relates to the challenge of learning disentangled representations \citep{locatello2020object} when modes share significant structure.

\subsection{Slot Dropout as Solution}

We found that \textbf{slot dropout} ($p=0.3$) during training effectively prevents collapse. Table~\ref{tab:slot_dropout} compares TTA performance across configurations:

\begin{table}[h]
\centering
\caption{Effect of Slot Dropout on TTA Performance. Only slot dropout achieves positive improvement.}
\label{tab:slot_dropout}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Configuration} & \textbf{Base Return} & \textbf{TTA $\Delta$} & \textbf{Verdict} \\
\midrule
Slot Dropout ($p=0.3$) & 187.4 & \textbf{+11.4} & $\checkmark$ Works \\
Slot Dropout ($p=0.5$) & 258.8 & $-4.7$ & Too aggressive \\
Strong Regularization & 490.5 & $-3.0$ & Ceiling effect \\
Collapsed Default & 499.5 & $-4.8$ & No diversity \\
Diverse (reg. only) & 314.0 & $-64.6$ & Catastrophic \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Interpretation of each configuration}:

\paragraph{Slot Dropout ($p=0.3$): The sweet spot.} With 30\% dropout, the network cannot rely on any single slot and must distribute useful information across all three. The base return (187.4) is lower than collapsed models because the network cannot fully exploit the best slot, but this ``handicap'' during training enables meaningful TTA at test time.

\paragraph{Slot Dropout ($p=0.5$): Over-regularization.} At 50\% dropout, half the slots are masked at each step, forcing extremely distributed representations. While this achieves higher base return (258.8) than $p=0.3$, it may fragment learned behaviors too much, making gating weight adjustments less effective.

\paragraph{Strong Regularization: Ceiling effect.} High weight entropy and load balancing regularization achieves excellent base performance (490.5) with numerical diversity in weights. However, TTA yields $-3.0$---the model is already near-optimal, leaving no room for improvement. This is a \emph{good problem} but defeats the purpose of TTA.

\paragraph{Collapsed Default: No diversity to exploit.} Without regularization, the model collapses to a single slot and achieves near-maximum return (499.5). TTA cannot help ($-4.8$) because changing gating weights has no effect when one slot dominates completely.

\paragraph{Diverse (regularization only): Numerical but not functional diversity.} Regularization creates weight diversity (slots have different utilization), but the \emph{abstractions themselves} may encode similar behaviors. TTA then causes catastrophic interference ($-64.6$) as the gating network explores a space of functionally similar representations.

\paragraph{Key Insight.} Regularization creates \emph{numerical} diversity (slots have different weights) but not \emph{functional} diversity (slots encode different behaviors). Slot dropout forces the network to learn useful representations in \emph{all} slots, since any slot may be masked during training. This distinction---numerical vs. functional diversity---is crucial for effective TTA.

%==============================================================================
\section{Fair Comparison Experiments}
\label{sec:fair_comparison}
%==============================================================================

A critical challenge in evaluating \method{} is ensuring fair comparisons. Simply comparing gating-only adaptation ($\sim$4.3k params) against full fine-tuning ($\sim$51k params) conflates the effects of \emph{what} is being adapted with \emph{how much} is being adapted. We designed four experiments to isolate specific hypotheses about \method{}'s adaptation properties.

\subsection{Experiment A: Parameter-Matched Comparison}

\textbf{Question}: Is gating adaptation better than policy-head adaptation with the same parameter budget ($\sim$4,300 params)?

\textbf{Motivation}: This experiment controls for parameter count, isolating the effect of \emph{what} is adapted (gating weights vs. policy weights) rather than how many parameters are updated.

\textbf{Protocol}: Both methods adapt exactly $\sim$4,300 parameters for 30 episodes on each mode, then evaluate.

\begin{table}[h]
\centering
\caption{Experiment A: Parameter-Matched Comparison. Gating avoids catastrophic drops.}
\label{tab:exp_a}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Method} & \textbf{Mode 0 $\Delta$} & \textbf{Mode 1 $\Delta$} & \textbf{Mode 2 $\Delta$} & \textbf{Worst Case} \\
\midrule
Gating & $-2.7$ & $-7.9$ & $+14.7$ & $-7.9$ \\
Policy Head & $-2.4$ & $-105.0$ & $+10.0$ & $-105.0$ \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Results and Interpretation}: With matched parameters, gating demonstrates dramatically more stable adaptation. The policy-head approach catastrophically failed on Mode 1 ($-105.0$), while gating's worst case was only $-7.9$---a \textbf{13$\times$ reduction in worst-case degradation}.

\textbf{Why does this happen?} Policy-head parameters directly control action probabilities. When these weights shift to improve performance on one mode, they may become miscalibrated for others. In contrast, gating weights only determine \emph{which abstraction to use}---the abstractions themselves remain intact. Even if the gating network incorrectly selects an abstraction, the resulting behavior is still a valid learned policy, not a corrupted one.

\textbf{Implication}: The location of adaptation matters as much as the amount. Gating provides a natural ``safety barrier'' that policy-head adaptation lacks.

\subsection{Experiment B: Catastrophic Forgetting Test}

\textbf{Question}: How much do non-adapted modes degrade when adapting to one mode?

\textbf{Motivation}: This is the core test of \method{}'s forgetting resistance. In real deployments, an agent may need to adapt to a new condition while retaining competence on previously encountered ones. We measure the ``collateral damage'' of single-mode adaptation.

\textbf{Protocol}: (1) Evaluate baseline performance on all three modes. (2) Adapt exclusively to Mode 0 for 30 episodes. (3) Re-evaluate on all modes. (4) Compute ``forgetting'' as the performance drop on non-adapted modes.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{forgetting_comparison.png}
\caption{\textbf{Experiment B: Catastrophic Forgetting.} After adapting to Mode 0, gating preserves Mode 1 almost perfectly ($-0.2$) while full fine-tuning catastrophically forgets ($-143.5$). \textbf{Total forgetting: Gating $-25.8$ vs Full $-250.1$ (10$\times$ less).}}
\label{fig:forgetting}
\end{figure}

\begin{table}[h]
\centering
\caption{Experiment B: Catastrophic Forgetting (adapt Mode 0, evaluate all modes)}
\label{tab:exp_b}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Method} & \textbf{Mode 0 After} & \textbf{Mode 1 $\Delta$} & \textbf{Mode 2 $\Delta$} & \textbf{Total Forgot} \\
\midrule
Gating & 163.9 & $-0.2$ & $-25.6$ & $-25.8$ \\
Full Fine-Tune & 62.9 & $-143.5$ & $-106.6$ & $-250.1$ \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Results and Interpretation}: This experiment reveals the most striking difference between adaptation strategies:

\begin{itemize}
    \item \textbf{Gating preserves Mode 1 almost perfectly} ($-0.2$), demonstrating that adaptation to Mode 0 barely affects Mode 1 competence. The total forgetting of $-25.8$ is dominated by Mode 2 ($-25.6$), suggesting some mode pairs are more independent than others.
    
    \item \textbf{Full fine-tuning catastrophically destroys both non-adapted modes} ($-143.5$ on Mode 1, $-106.6$ on Mode 2). Remarkably, full fine-tuning also achieves \emph{worse} performance on the adapted mode (62.9 vs 163.9), suggesting that catastrophic forgetting can destabilize the entire policy, not just non-target modes.
\end{itemize}

\textbf{Why the 10$\times$ difference?} The gating network operates in a fundamentally different space than the policy. When gating weights shift toward Mode 0, they move \emph{away from} other modes in attention space, but the underlying abstractions remain unchanged. In contrast, when policy weights shift, they physically overwrite the parameters that encoded other behaviors.

\textbf{The ``anchor'' effect}: The frozen policy acts as a stable anchor. No matter how the gating weights change, the policy can only express behaviors that are convex combinations of the learned abstractions. This provides a \emph{structural guarantee} against catastrophic corruption that regularization-based methods (EWC, etc.) cannot match.

\textbf{Implication}: For multi-mode deployments where forgetting is unacceptable, gating-only adaptation is strongly preferred despite lower peak improvement.

\subsection{Experiment C: Few-Shot Adaptation Speed}

\textbf{Question}: Which method adapts most reliably with limited episodes?

\textbf{Motivation}: In practice, the adaptation budget is often constrained or unknown in advance. A robust method should perform reasonably across a wide range of episode budgets, without requiring careful tuning of when to stop adaptation.

\textbf{Protocol}: For each method, we run adaptation with budgets of 1, 3, 5, 10, 20, and 30 episodes, measuring improvement over the unadapted baseline.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{fewshot_adaptation.png}
\caption{\textbf{Experiment C: Few-Shot Adaptation.} Gating (blue) shows consistent positive improvement for budgets 1-20, never catastrophically failing. Policy-head (orange) peaks at 5 episodes (+155.7) but collapses after 20 episodes ($-122.7$). Full fine-tuning (red) is highly unstable.}
\label{fig:fewshot}
\end{figure}

\begin{table}[h]
\centering
\caption{Experiment C: Few-Shot Adaptation (improvement vs baseline)}
\label{tab:exp_c}
\begin{tabular}{@{}lcccccc@{}}
\toprule
\textbf{Method} & \textbf{1 ep} & \textbf{3 ep} & \textbf{5 ep} & \textbf{10 ep} & \textbf{20 ep} & \textbf{30 ep} \\
\midrule
Gating & $+12.1$ & $+11.7$ & $+4.2$ & $+15.2$ & $+9.0$ & $-6.3$ \\
Policy Head & $+3.5$ & $+50.6$ & $+155.7$ & $-19.0$ & $-58.1$ & $-122.7$ \\
Full & $-50.4$ & $+21.3$ & $-10.4$ & $-4.0$ & $-78.0$ & $+1.1$ \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Results and Interpretation}: This experiment reveals fundamentally different adaptation dynamics:

\paragraph{Gating (Blue): Consistent and Safe.} Gating achieves positive improvement for budgets 1--20, with remarkably low variance ($+4.2$ to $+15.2$). Even at 30 episodes, the degradation ($-6.3$) is mild. This consistency means practitioners can deploy gating adaptation without carefully tuning the stopping criterion---it will likely help and almost certainly won't catastrophically fail.

\paragraph{Policy-Head (Orange): High Risk, High Reward.} Policy-head adaptation exhibits a dramatic peak at exactly 5 episodes ($+155.7$)---\textbf{14$\times$ higher than gating's best}. However, this peak is followed by rapid collapse: at 30 episodes, performance drops to $-122.7$. This pattern suggests policy-head adaptation initially finds useful gradient directions but quickly overfits, destroying the generalization properties of the original policy.

\textbf{The ``Goldilocks problem''}: To use policy-head adaptation effectively, one must know \emph{exactly} when to stop. Too few episodes underutilizes the potential; too many causes catastrophic overfitting. In practice, this optimal stopping point is unknown and may vary across modes and environments.

\paragraph{Full Fine-Tuning (Red): Chaotic.} Full fine-tuning shows no consistent pattern: $-50.4$ at 1 episode, $+21.3$ at 3, $-10.4$ at 5, etc. This unpredictability makes it unsuitable for any deployment where reliability matters.

\textbf{Why is gating so stable?} The gating network has a naturally bounded influence: it can only select among existing abstractions. Even with extensive training, it cannot ``overfit'' in the traditional sense because the action distribution is ultimately determined by the frozen policy head operating on fixed abstractions. This provides an implicit regularization that policy-head and full fine-tuning lack.

\textbf{Implication}: For deployments with unknown or variable adaptation budgets, gating is the only method that provides reliable improvement without catastrophic risk.

\subsection{Experiment D: Extreme Mode Differences}

\textbf{Question}: How do methods perform when environment modes are dramatically different?

\textbf{Motivation}: Our standard CartPole modes have relatively subtle differences (gravity 9.8--10.8, length 0.4--0.6). One might hypothesize that gating's advantage diminishes when modes are more distinct, as full fine-tuning could leverage its greater capacity to learn mode-specific behaviors. We test this by creating an ``extreme'' environment.

\textbf{Protocol}: We modify CartPole to have extreme parameter ranges: gravity 5.0--25.0 (5$\times$ range vs. 1.1$\times$) and pole length 0.3--0.8 (2.7$\times$ range vs. 1.5$\times$). These differences require fundamentally different control strategies.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{extreme_modes.png}
\caption{\textbf{Experiment D: Extreme Modes.} On harder mode differences, gating maintains positive average improvement ($+8.5$) while full fine-tuning shows catastrophic failure on Mode 1 ($-103.0$).}
\label{fig:extreme}
\end{figure}

\begin{table}[h]
\centering
\caption{Experiment D: Extreme Mode Differences (gravity 5--25, length 0.3--0.8)}
\label{tab:exp_d}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Method} & \textbf{Mode 0 $\Delta$} & \textbf{Mode 1 $\Delta$} & \textbf{Mode 2 $\Delta$} & \textbf{Average} \\
\midrule
Gating & $+27.6$ & $-0.4$ & $-1.8$ & $+8.5$ \\
Full Fine-Tune & $+124.9$ & $-103.0$ & $-27.6$ & $-1.8$ \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Results and Interpretation}: Counter to the hypothesis that extreme differences would favor full fine-tuning, we observe the opposite pattern:

\paragraph{Gating scales gracefully.} Gating achieves $+27.6$ on the adapted mode (Mode 0) while maintaining near-zero degradation on others ($-0.4$, $-1.8$). The average improvement of $+8.5$ demonstrates that gating's stability advantage \emph{increases} with mode diversity.

\paragraph{Full fine-tuning's catastrophic failure amplifies.} Full fine-tuning achieves impressive single-mode improvement ($+124.9$ on Mode 0) but at devastating cost: $-103.0$ on Mode 1 and $-27.6$ on Mode 2. The net result is \emph{negative} average improvement ($-1.8$). More extreme modes mean more distinct optimal policies, making interference between modes more severe.

\textbf{Why does extreme diversity favor gating?} With subtle mode differences, a policy can partially generalize across modes---adapting to one doesn't completely destroy competence on others. With extreme differences, modes require genuinely different behaviors. Full fine-tuning must choose which behavior to encode, necessarily sacrificing others. Gating sidesteps this by keeping all behaviors frozen and merely selecting among them.

\textbf{The capacity argument revisited}: One might argue full fine-tuning should excel with extreme modes because it has more parameters to represent mode-specific behaviors. However, \emph{having capacity} is different from \emph{using capacity wisely}. Full fine-tuning's gradient updates optimize for the current mode without any mechanism to preserve others. The greater the mode differences, the more destructive these updates become.

\textbf{Implication}: \method{}'s stability advantage is not limited to similar modes---it actually \emph{increases} as modes become more different, precisely the regime where robust adaptation matters most.

\subsection{Summary of Fair Comparison Findings}

Across all four experiments, a consistent pattern emerges:

\begin{table}[h]
\centering
\caption{Summary: Gating vs. Alternatives Across All Experiments}
\label{tab:summary}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Metric} & \textbf{Gating} & \textbf{Best Alternative} \\
\midrule
Worst-case single-mode drop & $-7.9$ & $-105.0$ (Policy Head) \\
Total forgetting (Exp B) & $-25.8$ & $-250.1$ (Full) \\
Catastrophic failures (Exp C) & 0/6 budgets & 4/6 budgets (Full) \\
Average improvement (Extreme) & $+8.5$ & $-1.8$ (Full) \\
\bottomrule
\end{tabular}
\end{table}

\textbf{The core tradeoff}: Gating sacrifices peak single-mode improvement (policy-head achieved $+155.7$ at its optimal budget) in exchange for:
\begin{enumerate}
    \item \textbf{10$\times$ less forgetting} on non-adapted modes
    \item \textbf{Zero catastrophic failures} across all tested conditions
    \item \textbf{Consistent performance} regardless of adaptation budget
    \item \textbf{Graceful scaling} to more extreme mode differences
\end{enumerate}

This tradeoff is not always favorable---for single-mode optimization with known optimal stopping, policy-head adaptation is superior. But for the multi-mode, risk-averse deployment scenarios that motivate \method{}, gating's stability is the critical property.

%==============================================================================
\section{Analysis}
\label{sec:analysis}
%==============================================================================

\subsection{Why Gating Provides Stability}

The frozen policy acts as an ``anchor'' that cannot be corrupted during adaptation, similar to the frozen pretrained weights in adapter-based fine-tuning \citep{rusu2016progressive}. The gating network can only \emph{reweight} existing abstractions---it cannot create new behaviors or destroy learned ones. This structural constraint limits both upside (maximum improvement) and downside (catastrophic failure), trading off with methods that allow full parameter updates \citep{finn2017maml}.

Formally, let $\pi_\theta(a|s) = f(\bh'_t)$ where $\bh'_t = \gamma(\bz_t) \odot \bh_t + \beta(\bz_t)$ \citep{perez2018film} and $\bz_t = \sum_k w_k \mathbf{a}_k$. During gating-only adaptation, only $\{w_k\}$ changes while $\{\mathbf{a}_k\}$, $f$, $\gamma$, $\beta$ remain fixed. The policy is constrained to the convex hull of behaviors encoded by the fixed slot vectors, providing a geometric interpretation of the stability guarantee.

\subsection{When to Use \method{}}

\begin{table}[h]
\centering
\caption{When to Use Each Adaptation Strategy}
\label{tab:when_to_use}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Scenario} & \textbf{Best Method} & \textbf{Reason} \\
\midrule
Multi-mode deployment & Gating & Prevents forgetting other modes \\
Unknown adaptation budget & Gating & Consistent across all budgets \\
Risk-averse setting & Gating & No catastrophic failures observed \\
\midrule
Single-mode optimization & Full Fine-Tune & Maximum improvement possible \\
Exactly 5 episodes available & Policy Head & Peak performance at this budget \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Training Performance}

Figure~\ref{fig:training} shows that \method{} successfully learns the task, achieving comparable performance to the well-tuned SB3 PPO baseline.

\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{training_comparison_v2.png}
\caption{\textbf{Training Performance.} ASTRAL (best config) achieves $\sim$490 return, comparable to SB3 PPO ($\sim$443). The original GRU baseline failed to learn ($\sim$24), indicating the abstraction bank is beneficial for learning.}
\label{fig:training}
\end{figure}

\subsection{Limitations}

\begin{enumerate}
    \item \textbf{Lower peak improvement}: Full fine-tuning can achieve $+77$ improvement (SB3 baseline) vs gating's $+11$.
    \item \textbf{Requires slot dropout}: Without it, slot collapse prevents meaningful TTA.
    \item \textbf{Single environment}: Results demonstrated on CartPole; generalization to complex environments requires further study.
    \item \textbf{No mode-slot correspondence}: Slots do not cleanly map to environment modes, limiting interpretability.
\end{enumerate}

%==============================================================================
\section{Discussion}
\label{sec:discussion}
%==============================================================================

\subsection{The Stability-Performance Tradeoff}

Our experiments reveal a fundamental tension in test-time adaptation, echoing broader findings in the continual learning literature \citep{parisi2019continual}:

\begin{quote}
\emph{Methods that achieve higher peak improvement also exhibit higher variance and more frequent catastrophic failures. Stability and maximum improvement are fundamentally in tension.}
\end{quote}

Full fine-tuning can achieve $+77$ improvement on a single mode (SB3 baseline experiment) but causes $-143$ forgetting on other modes. Gating achieves only $+11$ improvement but with $-0.2$ forgetting---a 10$\times$ reduction in catastrophic risk. This tradeoff is consistent with theoretical analyses of plasticity-stability dilemmas in neural networks \citep{mccloskey1989catastrophic, french1999catastrophic}.

\subsection{Why Does Slot Collapse Occur?}

We hypothesize several contributing factors, drawing parallels to related phenomena in multi-task learning and MoE architectures \citep{shazeer2017outrageously, jacobs1991adaptive}:

\paragraph{Reward Signal.} CartPole provides dense reward (+1 per step), but mode information is implicit in dynamics. The agent can achieve high return without distinguishing modes explicitly. This relates to the challenge of learning disentangled representations without explicit supervision \citep{locatello2020object}.

\paragraph{Optimization Landscape.} Policy gradient methods \citep{schulman2017proximal} are greedy; once a slot achieves good performance, gradients reinforce its usage, creating a ``rich get richer'' dynamic similar to winner-take-all effects in competitive learning \citep{jacobs1991adaptive}.

\paragraph{Insufficient Mode Diversity.} The three CartPole modes may be too similar, allowing a single generalist policy to perform well across all. More diverse environments might naturally encourage slot specialization, as observed in multi-task RL settings \citep{sodhani2021multi}.

\subsection{Implications for Adaptive RL}

The slot collapse problem reveals that:

\begin{quote}
\emph{The objective (maximizing return) does not inherently require slot specialization. Any regularization that enforces diversity must trade off against task performance.}
\end{quote}

This suggests that purely self-supervised approaches to learning modular abstractions in RL may be insufficient. Future work may require explicit mode supervision, curriculum learning, or architectural constraints that enforce specialization.

%==============================================================================
\section{Conclusion}
\label{sec:conclusion}
%==============================================================================

We introduced \method{}, an architecture for stable test-time adaptation in non-stationary reinforcement learning. Through extensive experimentation (33+ models, 4 fair comparison experiments), we demonstrated that:

\begin{itemize}
    \item \textbf{Slot collapse} is the primary failure mode of abstraction banks, but \textbf{slot dropout} ($p=0.3$) effectively mitigates it.
    \item Gating-only adaptation provides \textbf{10$\times$ less catastrophic forgetting} compared to full fine-tuning ($-25.8$ vs $-250.1$).
    \item Gating maintains \textbf{consistent, low-variance performance} across all episode budgets (1-50), while alternatives show high variance with frequent catastrophic failures.
    \item \textbf{Stability and maximum improvement are in tension}: full fine-tuning achieves higher peaks but with unacceptable forgetting risk.
\end{itemize}

We recommend \method{} for \emph{risk-averse} deployments where maintaining competence across all environment modes is more critical than maximizing single-mode performance. Future work should explore functional diversity mechanisms beyond slot dropout \citep{srivastava2014dropout}, explicit mode-conditioned training \citep{zintgraf2020varibad}, extension to more complex environments \citep{brockman2016gym}, and integration with meta-learning approaches \citep{finn2017maml, rakelly2019efficient} that could provide complementary benefits.

\paragraph{Broader Impact.} Safe adaptation without forgetting is critical for real-world RL deployment \citep{kumar2021rma}. \method{} provides a step toward agents that can adapt to changing conditions without catastrophic failures, important for robotics \citep{tobin2017domain}, autonomous vehicles, and other safety-critical applications where stability is paramount \citep{verma2018programmatically}.

%==============================================================================
\bibliographystyle{unsrtnat}
\bibliography{references}

%==============================================================================
\appendix
\section{Additional Experimental Details}
\label{sec:appendix}
%==============================================================================

\subsection{TTA Performance by Model Type}

Figure~\ref{fig:tta_models} shows TTA improvement across all tested model configurations. Only slot dropout ($p=0.3$) achieves positive improvement.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{tta_by_model.png}
\caption{TTA improvement by model configuration. Slot dropout (0.3) is the \textbf{only} method achieving positive TTA improvement.}
\label{fig:tta_models}
\end{figure}

\subsection{Causal Intervention Analysis}

To understand slot specialization, we performed causal interventions on trained models (Figure~\ref{fig:interventions}):

\begin{itemize}
    \item \textbf{Clamping}: Force 100\% weight to a single slot
    \item \textbf{Disabling}: Zero out a slot and redistribute weight
\end{itemize}

Results show that performance is relatively robust to slot manipulation in diverse models, but collapsed models are highly sensitive.

\begin{figure}[h]
\centering
\includegraphics[width=0.95\textwidth]{causal_interventions.png}
\caption{Causal intervention experiments showing effect of clamping (left) and disabling (right) individual slots.}
\label{fig:interventions}
\end{figure}

\subsection{SB3 PPO Fine-Tuning Baseline}

Figure~\ref{fig:sb3} shows that the SB3 PPO baseline can be successfully fine-tuned, achieving $+77$ average improvement. This demonstrates that adaptation \emph{is} possible with full fine-tuning---but at the cost of forgetting.

\begin{figure}[h]
\centering
\includegraphics[width=0.75\textwidth]{sb3_finetuning.png}
\caption{SB3 PPO fine-tuning achieves $+77$ average improvement, demonstrating that full adaptation is possible---but causes forgetting on non-adapted modes.}
\label{fig:sb3}
\end{figure}

\subsection{Computational Resources}

All experiments were run on a single NVIDIA RTX 4090 GPU. Training times:
\begin{itemize}
    \item 100k steps: $\sim$1 minute
    \item 300k steps: $\sim$3 minutes
    \item 500k steps: $\sim$5 minutes
\end{itemize}
Total compute for all 33+ models: approximately 3 GPU-hours.

\end{document}
