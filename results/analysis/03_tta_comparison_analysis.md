# TTA Comparison Analysis: ASTRAL vs Baseline

**Date:** December 2, 2025
**Experiment:** Baseline Control Comparison (from `09_experiments_tta_validation.md`)

---

## Summary

| Condition | Agent | Adapt Mode | Avg Before | Avg After | Avg Improvement |
|-----------|-------|------------|------------|-----------|-----------------|
| **ASTRAL Gating** | best_config_strong | gating | 494.6 | 491.4 | -3.2 |
| **Baseline Policy** | baseline_42 | policy_head | 23.8 | 21.9 | -1.9 |
| **ASTRAL Policy** | best_config_strong | policy_head | 493.3 | 179.3 | -314.1 |

---

## Key Findings

### 1. ASTRAL with Gating TTA (Intended Approach)

| Mode | Before | After | Improvement |
|------|--------|-------|-------------|
| 0 | 499.25 | 490.80 | -8.45 |
| 1 | 494.35 | 483.40 | -10.95 |
| 2 | 490.35 | 500.00 | **+9.65** |

**Observation:** The `best_config_strong` model is already **near-optimal** (~495-500 returns). TTA maintains performance with minor fluctuations. Mode 2 shows slight improvement.

**Why no improvement?** The model already achieves maximum return (500), so there's no room to improve. TTA is designed to help when performance is suboptimal.

---

### 2. Baseline with Policy Head TTA (Control)

| Mode | Before | After | Improvement |
|------|--------|-------|-------------|
| 0 | 20.65 | 23.15 | +2.50 |
| 1 | 28.80 | 27.05 | -1.75 |
| 2 | 22.05 | 15.40 | -6.65 |

**Observation:** The baseline model performs **terribly** (~20-30 returns vs 500 max). TTA provides negligible improvement.

**Critical Issue:** This baseline model appears to have failed during training. Returns of 20-30 indicate the agent never learned the task properly. This makes the comparison unfair.

---

### 3. ASTRAL with Policy Head TTA (Ablation)

| Mode | Before | After | Improvement |
|------|--------|-------|-------------|
| 0 | 498.45 | 20.55 | **-477.90** ðŸ”´ |
| 1 | 486.75 | 500.00 | +13.25 |
| 2 | 494.75 | 17.25 | **-477.50** ðŸ”´ |

**Observation:** Catastrophic forgetting! Adapting the policy head destroyed performance on Modes 0 and 2.

**Why?** The policy head is tightly coupled with the abstraction bank. Changing it without the gating mechanism causes the agent to lose its learned behavior. This actually **supports the ASTRAL hypothesis** â€” the gating network is the right thing to adapt.

---

## Interpretation

### Why These Results Support ASTRAL

1. **Gating-only TTA is stable:** ASTRAL with gating adaptation maintains near-optimal performance
2. **Policy-head TTA is catastrophic:** Changing the policy head breaks the model (2/3 modes crash)
3. **Baseline comparison is confounded:** The baseline model never learned, so we can't fairly compare

### The Real Comparison Problem

The baseline model (`baseline_baseline_42`) has returns of ~20-30, while ASTRAL (`best_config_strong`) has returns of ~500. This 20x performance gap means:
- ASTRAL already solved the task
- The baseline failed to learn
- We can't meaningfully compare adaptation on a broken model

---

## Recommendations

### Immediate: Re-run with Better Baseline

We need a baseline that actually works. Check if any baseline model achieved reasonable performance:

```bash
# Find a better baseline
for run in results/runs/baseline_*; do
    echo "$run:"
    grep -o "mean_return=[0-9.]*" "$run/events.out.tfevents.*" | tail -1
done
```

Or train a new baseline with more steps or hyperparameter tuning.

### Alternative Analysis

Since both ASTRAL and baseline start at such different performance levels, consider:

1. **Use a common starting point:** Evaluate both on held-out modes where initial performance is similar
2. **Compare adaptation efficiency:** How quickly does each approach improve (even if from different baselines)?
3. **Compare relative improvement:** % improvement rather than absolute

---

## Conclusion

The experiment reveals an important insight: **gating-only TTA is the right approach for ASTRAL**. Policy-head TTA causes catastrophic forgetting.

However, the baseline comparison is **inconclusive** because the baseline model failed to learn the task. We need a functioning baseline to properly validate the TTA hypothesis.

### What We Proved âœ…
- ASTRAL gating TTA maintains performance on near-optimal models
- Policy-head TTA causes catastrophic forgetting on ASTRAL
- The abstraction bank + gating design is load-bearing

### What We Didn't Prove âŒ
- Whether ASTRAL TTA > Baseline TTA (baseline was broken)
- Whether gating TTA improves suboptimal models (model was already optimal)

---

---

## Additional Experiment: Suboptimal ASTRAL Model

Tested TTA on `interp_all` (training return ~177) to see if TTA helps when there's room to improve:

| Mode | Before | After | Improvement |
|------|--------|-------|-------------|
| 0 | 182.75 | 159.60 | **-23.15** |
| 1 | 233.45 | 180.15 | **-53.30** |
| 2 | 146.25 | 144.60 | -1.65 |

**Result:** TTA made performance **worse** on a suboptimal model too!

### Possible Explanations

1. **REINFORCE is high-variance:** Episode-by-episode policy gradient is noisy and can diverge
2. **30 episodes insufficient:** May need more episodes or different learning rate
3. **Gating adaptation isn't the bottleneck:** The issue might be in the abstraction bank itself, not the gating
4. **Collapsed slots:** If `interp_all` has slot collapse, there's nothing meaningful to reweight

### Weight Analysis

The weights during adaptation show:
- Slot 2 dominates (~0.5-0.6)
- Slots 0 and 1 are underused (~0.2-0.3 and ~0.1-0.2)
- This suggests **partial slot collapse** â€” similar to our earlier findings

---

## Revised Conclusion

### What the TTA Experiments Show

| Finding | Interpretation |
|---------|----------------|
| Near-optimal models stable | TTA doesn't break good models |
| Suboptimal models don't improve | TTA isn't helping learn mode-specific behavior |
| Baseline never learned | Can't compare (confounded) |
| Policy-head TTA catastrophic | Gating mechanism is load-bearing |

### The Real Issue

TTA assumes the abstraction bank has **diverse, specialized slots** that can be reweighted for different modes. But if slots are collapsed or not mode-specialized, TTA has nothing useful to adapt.

**This circles back to the slot collapse problem** â€” we need to solve slot specialization before TTA can provide value.

---

## Next Steps

1. **Focus on slot specialization first** â€” Experiments 2-7 in `08_experiments_slot_collapse.md`
2. **Revisit TTA after achieving diverse slots** â€” If slots specialize, TTA should work
3. **Consider alternative adaptation methods** â€” Meta-learning, entropy regularization during adaptation

