# ASTRAL: Abstraction-Slot Test-time Reweighting for Adaptation in Latent RL

A small-scale proof-of-concept for **structured abstractions in in-context RL**.

ASTRAL tests whether discrete, learnable "abstraction slots" can provide interpretable, mode-specific adaptation in non-stationary environments, with efficient test-time adaptation by updating only the gating network.

---

## ğŸ¯ Research Question

> Can we add structured abstractions to in-context RL that are:
> 1. **Interpretable** â€” different modes activate different abstraction slots
> 2. **Efficient** â€” test-time adaptation requires updating only the gating network
> 3. **Causal** â€” clamping/disabling slots produces predictable behavioral changes

---

## ğŸ—ï¸ Architecture

```
Input: (s_t, a_{t-1}, r_{t-1})
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Input MLP  â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    GRU      â”‚  â† In-context adaptation via hidden state
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
         h_t (context embedding)
           â”‚
           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                          â”‚
           â–¼                          â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Gating MLP  â”‚          â”‚  Abstraction   â”‚
    â”‚   g(h_t)    â”‚          â”‚  Bank A [KÃ—d]  â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                         â”‚
           â–¼                         â”‚
    w_t = softmax(logits/Ï„)          â”‚
           â”‚                         â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
              z_t = w_t^T Â· A  (combined abstraction)
                        â”‚
                        â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚    FiLM     â”‚  â† Forces dependency on abstraction
                 â”‚  Î³, Î² = f(z)â”‚
                 â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
              h'_t = Î³ âŠ™ h_t + Î²  (modulated context)
                        â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                       â”‚
            â–¼                       â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ Policy Head â”‚         â”‚ Value Head  â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Design Decisions:**
- **FiLM modulation** ensures the policy depends on abstractions (no bypass)
- **Soft attention** over K slots allows gradient-based learning
- **Test-time adaptation** updates only the gating network (4K parameters)

---

## ğŸ“ Project Structure

```
test_time_RL/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ astral_implementation_plan.md # Detailed implementation guide
â”œâ”€â”€ astral_proposal.md           # Research proposal
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ envs/
â”‚   â”‚   â””â”€â”€ nonstationary_cartpole.py  # 3-mode CartPole environment
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ abstraction_bank.py   # K learnable slots + gating
â”‚   â”‚   â”œâ”€â”€ film.py               # Feature-wise Linear Modulation
â”‚   â”‚   â””â”€â”€ astral_agent.py       # Full agent + baseline
â”‚   â”‚
â”‚   â”œâ”€â”€ losses.py                 # Regularization losses
â”‚   â”œâ”€â”€ train.py                  # PPO training loop
â”‚   â”œâ”€â”€ test_time_adapt.py        # TTA experiments
â”‚   â””â”€â”€ interventions.py          # Causal intervention experiments
â”‚
â”œâ”€â”€ cleanrl/                      # CleanRL reference (cloned)
â”‚   â””â”€â”€ venv/                     # Python virtual environment
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ runs/                     # Training runs + checkpoints
â”‚   â”œâ”€â”€ tta/                      # TTA experiment results
â”‚   â””â”€â”€ interventions/            # Intervention experiment results
â”‚
â””â”€â”€ configs/                      # (Optional) Config files
```

---

## ğŸš€ Quick Start

### 1. Setup

```bash
# Clone and enter directory
cd test_time_RL

# Activate virtual environment
source cleanrl/venv/bin/activate

# Verify installation
python -c "import torch; import gymnasium; print('Ready!')"
```

### 2. Train ASTRAL

```bash
# Train ASTRAL agent (500k timesteps, ~10 min on CPU)
python src/train.py --total_timesteps 500000

# Train baseline (GRU-only, for comparison)
python src/train.py --use_abstractions False --exp_name baseline
```

### 3. Test-Time Adaptation

```bash
# Run TTA experiment on all modes
python src/test_time_adapt.py \
    --checkpoint results/runs/<run_name>/final_model.pt \
    --num_adapt_episodes 30
```

### 4. Causal Interventions

```bash
# Run clamping/disabling experiments
python src/interventions.py \
    --checkpoint results/runs/<run_name>/final_model.pt \
    --num_episodes 20
```

### 5. View Logs

```bash
tensorboard --logdir results/runs
```

---

## ğŸ§ª Environment: NonStationaryCartPole

A CartPole variant with 3 hidden "modes" that change physical dynamics:

| Mode | Gravity | Pole Length | Difficulty |
|:-----|:--------|:------------|:-----------|
| 0 | 9.8 | 0.5 | Default |
| 1 | 7.5 | 0.7 | Easy (slower, longer) |
| 2 | 12.0 | 0.4 | Hard (faster, shorter) |

The agent does **not** observe the mode â€” it must infer it from dynamics.

---

## ğŸ“Š Key Results

### Training
- Both ASTRAL and baseline learn CartPole (~100-150 return)
- Mode 1 (easy) performs best, Mode 2 (hard) worst

### Test-Time Adaptation
- TTA improves Mode 0 by **+10.4%** with only 20 episodes
- Updates only **4,355 parameters** (8.5% of model)

### Slot Collapse (Known Issue)
All modes collapse to using **Slot 1** (~99.99%). This limits interpretability.

**Causal Evidence:**
- Clamping to Slot 1: Best performance
- Clamping to Slot 0/2: Severe drop (-75 to -115 points)
- Disabling Slot 1: Catastrophic failure

See `docs/interpretability_improvements.md` for solutions.

---

## ğŸ”§ Configuration

Key hyperparameters in `src/train.py`:

| Parameter | Default | Description |
|:----------|:--------|:------------|
| `d_model` | 64 | Hidden dimension |
| `num_abstractions` | 3 | Number of slots (K) |
| `tau` | 1.0 | Softmax temperature |
| `learning_rate` | 3e-4 | PPO learning rate |
| `lambda_w_ent` | 0.001 | Weight entropy regularization |
| `lambda_lb` | 0.001 | Load balancing regularization |
| `lambda_orth` | 0.0001 | Orthogonality regularization |

### Interpretability Improvements (Optional)

All improvements are modular and disabled by default:

| Flag | Description |
|:-----|:------------|
| `--use_gumbel True` | Gumbel-Softmax for slot exploration |
| `--hard_routing True` | Discrete one-hot slot selection |
| `--orthogonal_init True` | Initialize slots orthogonally |
| `--temp_anneal True` | Anneal temperature from high to low |
| `--tau_start 5.0` | Starting temperature (if annealing) |
| `--tau_end 0.5` | Ending temperature (if annealing) |
| `--lambda_contrast 0.01` | Contrastive loss (modeâ†’slot) |
| `--slot_prediction True` | Auxiliary slot prediction task |

---

## ğŸ“ˆ Experiments

### 1. Baseline Comparison
```bash
# Train both
python src/train.py --use_abstractions True --exp_name astral
python src/train.py --use_abstractions False --exp_name baseline

# Compare in tensorboard
tensorboard --logdir results/runs
```

### 2. Regularization Ablation
```bash
# Stronger regularization
python src/train.py --lambda_w_ent 0.01 --lambda_lb 0.01 --lambda_orth 0.001

# No regularization
python src/train.py --lambda_w_ent 0 --lambda_lb 0 --lambda_orth 0
```

### 3. Temperature Sweep
```bash
# Cold (peaked weights)
python src/train.py --tau 0.1

# Hot (uniform weights)
python src/train.py --tau 10.0
```

### 4. Interpretability Improvements
```bash
# All improvements (recommended for addressing slot collapse)
python src/train.py \
    --use_gumbel True \
    --hard_routing True \
    --orthogonal_init True \
    --temp_anneal True \
    --lambda_contrast 0.01 \
    --slot_prediction True

# Just temperature annealing
python src/train.py --temp_anneal True --tau_start 5.0 --tau_end 0.5

# Contrastive loss only
python src/train.py --lambda_contrast 0.05
```

---

## ğŸ“š Documentation

| Document | Description |
|:---------|:------------|
| `CONTEXT.md` | **Quick onboarding for new environments** |
| `docs/baseline_vs_astral.md` | Train vs Test-time differences |
| `docs/abstraction_bank_vs_moe.md` | Comparison with Mixture of Experts |
| `docs/experiment_guide.md` | Complete commands for all experiments |
| `docs/interpretability_improvements.md` | Solutions for slot collapse |

---

## ğŸ”¬ Future Work

1. **Fix Slot Collapse** â€” See `docs/interpretability_improvements.md`
2. **Scale to TAG-AMAGO** â€” Transformer backbone, MuJoCo/Meta-World
3. **Mode-Conditioned Auxiliary Loss** â€” Encourage modeâ†’slot correspondence
4. **Continual Learning** â€” Test on sequentially changing modes

---

## ğŸ“š References

- **FiLM**: Perez et al., "FiLM: Visual Reasoning with a General Conditioning Layer"
- **AMAGO**: Grigsby et al., "AMAGO: Scalable In-Context Reinforcement Learning"
- **CleanRL**: Huang et al., "CleanRL: High-quality Single-file Implementations of Deep RL Algorithms"

---

## ğŸ“ Citation

```bibtex
@misc{astral2024,
  title={ASTRAL: Abstraction-Slot Test-time Reweighting for Adaptation in Latent RL},
  author={...},
  year={2024},
  note={Proof-of-concept implementation}
}
```

---

## ğŸ“„ License

MIT License

